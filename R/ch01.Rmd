From **The Book**
#  Pinheiro, J.C., and Bates, D.M. (2000)
#  *Mixed-Effects Models in S and S-PLUS*, Springer.

- Copyright (2000) Jose Pinheiro and Douglas Bates (Book!!)
- Copyright (2018-22) Martin Maechler (transl. to `Rmd`; additional plots, etc)

# Chapter 1 ---  Linear Mixed-Effects Models: Basic Concepts and Examples

```{r, setup }
library(nlme)
if(!dev.interactive(TRUE)) pdf(file = 'ch01_plot.pdf')
options(width = 75  # MM: width 65 is too narrow
      , digits = 5
        ## contrast setting: as `S`, not default in `R` :
      ,  contrasts = c(unordered = "contr.helmert", # <= see below
                         ordered = "contr.poly") )
```

## 1.1 A Simple Example of Random Effects

```{r, Rail-simple}
Rail # --> (6 x 3 = 18 rows, 2 columns)
plot(Rail) # (plots nicely because it is a 'groupedData' obj. w/ extra info :)
str(Rail)  # shows *ordered* factor we don't want for the lm() ["fixed effects"]:
Rail.u <- within(Rail, Rail <- factor(Rail, ordered=FALSE))
str(Rail.u)
fm1Rail.lm <- lm( travel ~ 1, data = Rail ) # intercept only: μ^ = Ȳ.
summary(fm1Rail.lm)
```
A one-way (1-way) ANOVA model assumes "Rail effect",
$$ Y_{i,j} = \mu + \beta_i + \varepsilon_{i,j}, $$
with $i = 1,2,\dots,m\;(= 6)$, for $j=1,\dots,n_i\;( = 3)$,
for $n$ observations $n = \sum_{i=1}^{m} n_i$,
and the usual assumption
$$\varepsilon_{i,j} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2).$$


```{r, Rail-lm}
## if we have an intercept, cannot use all 6 levels (singular matrix):
fm2.Rail.lm <- lm( travel ~ Rail, data = Rail.u )
# Above have set "helmert" contrast [R's default is "treatment"'": 1st level := 0]
summary(fm2.Rail.lm)
## In this case of 1-way ANOVA: Nicer to drop the intercept (`- 1`):
fm2Rail_no_I <- lm( travel ~ Rail - 1, data = Rail.u )
summary(fm2Rail_no_I)
anova(fm2Rail_no_I)
```
and the Rail effect _is_ significant.

### Excursion: Contrasts, model matrices for linear models `lm`:
```{r, Rail-X-mat}
X  <- model.matrix(fm2.Rail.lm) # "helmert"
X. <- model.matrix(fm2Rail_no_I)# no intercept
Xs <- model.matrix(lm( travel ~ C(Rail, sum),       data = Rail.u )) # "sum"
Xt <- model.matrix(lm( travel ~ C(Rail, treatment), data = Rail.u )) # "treatment"
require("Matrix")
Matrix(X , sparse=TRUE) # "helmert"   also: as(X, "sparseMatrix")
Matrix(X., sparse=TRUE) # no intercept
zapsmall(crossprod(X)) # =  X'X = t(X) %*% X  but more efficiently
## X'X diagonal <==> X has orthogonal columns <==> all fixed effects are uncorrelated
Matrix(crossprod(X.))
shortNms <- function(mat) { colnames(mat) <- abbreviate(colnames(mat)); mat }
Matrix(shortNms(Xs), sparse=TRUE) # "sum"       contrasts
Matrix(shortNms(Xt), sparse=TRUE) # "treatment" contrasts: beta_1 == 0
```


### Random effects

Alternatively, as the rails are
randomly selected from a large number of possible rails, we consider a
__random__ Rail effect,

$$ Y_{i,j} = \mu + b_i + \varepsilon_{i,j}, $$
for $i = 1,2,\dots,m$; $j=1,\dots,n_i$, where $b_i \sim \mathcal{N}(0,
\sigma_b^2)$ and the $b_i$ and $\varepsilon_{i,j}$ are independent.

Instead of the $m-1$ parameters $\beta_i$ ("-1" via a contrast setting),
have only one parameter $\sigma_b^2$ here:
```{r, Rail-lmE}
fm1Rail.lme <- lme(travel ~ 1, data = Rail, random = ~ 1 | Rail)
summary( fm1Rail.lme )
fm1Rail.lmeML <- update( fm1Rail.lme, method = "ML" )
summary( fm1Rail.lmeML )
plot( fm1Rail.lme )   # produces Figure 1.4
intervals( fm1Rail.lme )
anova( fm1Rail.lme )
```

But we can get at the __predicted__ random effects, too via `ranef(.)`, and
here, their empirical mean is equal to the theoretical $E[.] = 0$.
```{r, ranef}
ranef( fm1Rail.lme ); cat("sum(ranef(.)): ", round( sum(ranef(fm1Rail.lme)), 4),"\n")
```

How do the predicted random effects compare with the corresponding fixed
effects?  This is useful, if we use a __third__ way to specify contrasts
for the fixed effects:  `sum` contrasts ensure $\sum_{k=1}^6 \beta_k = 0$:
Instead of using `options(constrasts = ...)`, use `C(*, sum)` in formula:
```{r, ranef-fixef}
fm.Rail.lm.sum <- lm( travel ~ C(Rail, sum), data = Rail.u )
(dcf <- dummy.coef(fm.Rail.lm.sum)[[2]])
(re.fe <- cbind(RE = ranef(fm1Rail.lme)[[1]], FE = dcf))
matplot(re.fe, xaxt = "n", main="random and fixed effects") ; abline(h=0, lty=3)
legend("topleft", col=1:2, c("random", "fixed"), pch=as.character(1:2), bty="n")
axis(1, at=1:6, labels = rownames(re.fe))
```

the differences are so small, you don't see them in the plot (unless you
look *very* carefully ...
```{r, re-fe-diff}
cbind(re.fe, D = re.fe[,1] - re.fe[,2]) # *systematic* difference
```

Indeed, the random effects are slightly smaller in absolute value, or,
expressed more mathematically, the random effects look like the fixed
effects, "shrunken towards zero".
We will see that this is really what happens much more generally.


## 1.2 A Randomized Block Design

```{r, ergoStool-ex}
plot.design( ergoStool )   # produces Figure 1.6
plot(ergoStool) ##-> 4 Types of stool, 9 subject
contrasts( ergoStool$Type ) # treatments
ergoStool1 <- ergoStool[ ergoStool$Subject == "1", ]
model.matrix( effort ~ Type, ergoStool1)  # X matrix for Subject 1 ~~ contrasts!
fm1Stool <-
  lme(effort ~ Type, data = ergoStool, random = ~ 1 | Subject)
summary( fm1Stool )
zapsmall( vcov(fm1Stool) ) # Helmert contrasts ==> uncorrelated coef.
anova( fm1Stool )
op <- options( contrasts = c(factor = "contr.treatment", ## the defaults in R:
                             ordered = "contr.poly" ) )
contrasts( ergoStool$Type )
fm2Stool <-
  lme(effort ~ Type, data = ergoStool, random = ~ 1 | Subject)
summary( fm2Stool )
anova( fm2Stool )
model.matrix( effort ~ Type - 1, ergoStool1 )
fm3Stool <-
 lme(effort ~ Type - 1, data = ergoStool, random = ~ 1 | Subject)
summary( fm3Stool )
anova( fm3Stool )
intervals( fm1Stool )
plot( fm1Stool,   # produces Figure 1.8
      form = resid(., type = "p") ~ fitted(.) | Subject,
      abline = 0 )
options(op) # reset in order to reproduce "the book"
```

## 1.3  Mixed-effects Models for Replicated, Blocked Designs

```{r, Machines-plot}
plot(Machines)
```

Here, we have a 2-way anova (fixed eff `Machine`, random eff `Worker`),
*with* replications (3 each), a (main effect) model being

$$ Y_{i,j,k} = \mu + \beta_j + b_i + \varepsilon_{i,j,k}, $$
and $ i = 1,2,\dots,9$   $j=1,2,3$;  $k=1,2,3$.

If this model was correct, the `interaction.plot()` which plots the mean
(over $k$) for each $(i,j$) should show __parallel__ polylines,
```{r, Mach-interaction-plot}
with(Machines, interaction.plot( Machine, Worker, score, las = 1))   # Figure 1.10
```
Fitting a mixed effects model now,
```{r, Machines-lme}
fm1Machine <-
  lme( score ~ Machine, data = Machines, random = ~ 1 | Worker )
fm1Machine
```
As we have replicates for each $(i,j)$ we can estimate a (random) __interaction__
```{r, Machines+int}
fm2Machine <- update( fm1Machine, random = ~ 1 | Worker/Machine )
summary(fm2Machine)
anova( fm1Machine, fm2Machine )
```
which looks significantly better than the main-effect only term.
The model fit `fm2Machine` corresponds to the mathematical model

$$ Y_{i,j,k} = \mu + \beta_j + b_i + \tilde b_{i,j} + \varepsilon_{i,j,k}.$$


### 1.3.2  _Unbalanced_ Data
```{r, Unbal-Mach-ex}
 ## delete selected rows from the Machines data
 MachinesUnbal <- Machines[ -c(2,3,6,8,9,12,19,20,27,33), ]
 ## check that the result is indeed unbalanced now:
 table(MachinesUnbal$Machine, MachinesUnbal$Worker)

 fm1MachinesU <- lme( score ~ Machine, data = MachinesUnbal,
                    random = ~ 1 | Worker/Machine )
 fm1MachinesU
  ## intervals( fm1MachinesU )
```
Trying to do the same for the `ergoStool` data (which has *no* replicates):
```{r, ergoSt-interaction}
 fm4Stool <- lme( effort ~ Type, ergoStool, ~ 1 | Subject/Type )
 ##if (interactive())
     intervals(fm4Stool) ## looks suspicious
 (fm1Stool$sigma)^2
 (fm4Stool$sigma)^2 + 0.79621^2
```

```{r, Machines-worker-1}
 Machine1 <- Machines[ Machines$Worker == "1", ]
 model.matrix( score ~ Machine, Machine1 )  # fixed-effects  X_i
 model.matrix( ~ Machine - 1,   Machine1 )  # random-effects Z_i
 fm3Machine <- update( fm1Machine, random = ~Machine - 1 |Worker)
 summary( fm3Machine )
 anova( fm1Machine, fm2Machine, fm3Machine )
```

## 1.4  An Analysis of Covariance Model

### 1.4.1  Modelling Simple Linear Growth Curves

- also known as  _"Repeated Measurements"_ / _"longitudinal data"_

```{r, Orthodont-ex}
'?Orthodont' # look at help page
summary(Orthodont)
formula(Orthodont) # it is a "groupedData" object -> has formula() and ....
str    (Orthodont) # all kind of stuff
plot(Orthodont) # really shows
OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
fm1OrthF.lis <- lmList( distance ~ age, data = OrthoFem )
coef( fm1OrthF.lis )
intervals( fm1OrthF.lis )
plot( intervals ( fm1OrthF.lis ) )   # produces Figure 1.12
fm2OrthF.lis <- update( fm1OrthF.lis, distance ~ I( age - 11 ) )
plot( intervals( fm2OrthF.lis ) )    # produces Figure 1.13
fm1OrthF <-
  lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
summary( fm1OrthF )
fm1OrthFM <- update( fm1OrthF, method = "ML" )
summary( fm1OrthFM )
fm2OrthF <- update( fm1OrthF, random = ~ age | Subject )
anova( fm1OrthF, fm2OrthF )
random.effects( fm1OrthF )
ranef( fm1OrthFM )
coef( fm1OrthF )
plot( compareFits(coef(fm1OrthF), coef(fm1OrthFM)))   # Figure 1.15
plot( augPred(fm1OrthF), aspect = "xy", grid = TRUE )   # Figure 1.16
```

## 1.5  Models for Nested Classification Factors

```{r, Pixel-ex}
fm1Pixel <- lme( pixel ~ day + I(day^2), data = Pixel,
                random = list( Dog = ~ day, Side = ~ 1 ) )
intervals( fm1Pixel )
plot( augPred( fm1Pixel ) )   # produces Figure 1.18
VarCorr( fm1Pixel )
summary( fm1Pixel )
fm2Pixel <- update( fm1Pixel, random = ~ day | Dog)
anova( fm1Pixel, fm2Pixel )
fm3Pixel <- update( fm1Pixel, random = ~ 1 | Dog/Side )
anova( fm1Pixel, fm3Pixel )
fm4Pixel <- update( fm1Pixel, pixel ~ day + I(day^2) + Side )
summary( fm4Pixel )
```

## 1.6  A Split-Plot Experiment

```{r, split-plot:Oats-ex}
fm1Oats <- lme( yield ~ ordered(nitro) * Variety,
               random = ~ 1 | Block/Variety,
               data = Oats)
anova( fm1Oats )
fm2Oats <- update( fm1Oats, yield ~ ordered(nitro) + Variety )
anova( fm2Oats )
summary( fm2Oats )
fm3Oats <- update( fm1Oats, yield ~ ordered( nitro ) )
summary( fm3Oats )
fm4Oats <-
  lme( yield ~ nitro, data = Oats, random = ~ 1 | Block/Variety )
summary( fm4Oats )
VarCorr( fm4Oats )
intervals( fm4Oats )
plot(augPred(fm4Oats), aspect = 2.5, layout = c(6, 3),
     between = list(x = c(0, 0, 0.5, 0, 0))) # produces Figure 1.21
```

### cleanup

```{r }
proc.time()
```
